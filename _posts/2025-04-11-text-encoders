---
layout: post
title: "Text Encoders"
categories: blog
permalink: /text-encoders/
---

Text encoders are predominantly **encoder-only transformers**. We'll brieftly discuss how they are trained, without going too deep into architectural details. If you need an embedding model for your project, check the [Hugging Face embedding leaderboard](https://huggingface.co/spaces/mteb/leaderboard) to guide your decisions.

The purpose of encoder-only models is to produce meaningful embeddings, and there are two notable ways of achieving it. Both of them are **self-supervised**, that is allow to train model on synthetic supervised tasks, without external labeling.

# Strategy 1: synthetic classification tasks

A famous example of this strategy is the [**BERT**](https://arxiv.org/abs/1810.04805) model, developed by Google. It was trained on two tasks:

- *Masked Language Modeling*: prediction of masked tokens,
- *Next Sentence Prediction*: checking whether two sentences could be together in a text.

![]({{ site.baseurl }}/assets/images/text-encoders/bert-training.png){: .responsive-image style="--img-desktop:75%; --img-mobile:90%;"}

[Source](https://arxiv.org/abs/1810.04805)

Further models were often trained for only Masked Language Modeling.

BERT had many descendants; the most recent of them is [ModernBERT](https://arxiv.org/pdf/2412.13663) published in December 2024, which adopted many architectural novelties used in recent LLMs such as

-	Removal of bias terms in all linear layers except for the final unembedding layer,
-	Fancy GeGLU activations,
-	Rotary positional embeddings,
-	Flash attention,
-	Three-step training on larger and larger context length to achieve long-context (160K tokens) performance.

See the [LLM architecture](https://nebius-academy.github.io/knowledge-base/transformer-architectures/#) long read for more details about these features.

BERT-like models are mostly popular as bases for fine-tuning for classification, regression and similar tasks. However, they can also be used as encoders in vector stores.

# Strategy 2: directly establishing coherence between semantic similarity and geometric proximity

## Contrastive training

**Contrastive training** operates with a dataset of tuples $(q, d^+, \mathcal{D}^-)$, where 

- $q$ is the query.
- $d^+$ is the **positive pair**, that is a document which is relevant to the query. (Here "pair" refers to the fact that $q$ and $d^+$ are paired with each other.)
- $\mathcal{D}^- = \{d^-_1,\ldots,d^-_n\}$ are the **negative pairs**, that is documents which are irrelevant to the query.

The most popular loss function for contrastive training is the **InfoNCE loss** (introduced in [this paper](https://arxiv.org/pdf/1807.03748)):
$$\mathcal{L} = - \log{
\frac{e^{s(q, d^+)/\tau}}{e^{s(q, d^+)/\tau + \sum_{i=1}^ne^{s(q, d^-_i)/\tau}
},$$
where $\tau$ is the temperature parameter, and $s(q,d)$ estimates similarity between the embeddings of $q$ and $d$; for example, it can be cosine distance.

The trickier part is, however, where to get data and which documents to pair as positive and negative. There are some :

* Search - for example, the classic [MS MARCO](https://arxiv.org/pdf/1611.09268),
* Q&A,
* FEVER (Fact Extraction and VERification) consists of 185,445 claims generated by altering sentences extracted from Wikipedia and subsequently verified without knowledge of the sentence they were derived from. The claims are classified as Supported, Refuted or NotEnoughInfo



## Two-step training


**Sentence transformer** models are trained exactly for that! As an example, let's look at how the seminal [**Sentence-BERT** (**SBERT**)](https://arxiv.org/pdf/1908.10084) was trained. Architecturally, it's just a BERT model with pooling on top of it. The two instances of the BERT are shown in red boxes:

![]({{ site.baseurl }}/assets/images/text-encoders/sbert-training.png){: .responsive-image style="--img-desktop:40%; --img-mobile:90%;"}

[Source](https://arxiv.org/pdf/1908.10084)

SBERT was trained using two Natural Language Inference (NLI) datasets, [SNLI](https://aclanthology.org/D15-1075.pdf) and [MultiNLI](https://aclanthology.org/N18-1101.pdf) which contain sentence pairs labeled as contradiction, entailment (the logical consequence), or neutral. Here are some examples:

![]({{ site.baseurl }}/assets/images/text-encoders/nli-dataset-example.png){: .responsive-image style="--img-desktop:90%; --img-mobile:90%;"}

[Source](https://aclanthology.org/D15-1075.pdf)

The sentence pairs (Sentence A, Sentence B) are classified in the following way:

1. First, we take two sentences and pass them through SBERT, which is a combination of BERT and a pooling layer. This process gives us two vector embeddings, let's call them **u** and **v**.
2. Next, we combine these two vectors, **u** and **v**, along with their elementwise absolute difference, which is written as **|u - v|**. This helps us capture the differences between the two sentences.
3. Then, we take this combined vector **[u, v, |u - v|]** and multiply it by a weight matrix **W**. This step transforms our combined vector into a new vector of length 3, representing the logits for our three classes: contradiction, entailment, and neutral.
4. Finally, we apply a **softmax** function to this new vector to obtain the probabilities.

## Strategy 3: contrastive training


